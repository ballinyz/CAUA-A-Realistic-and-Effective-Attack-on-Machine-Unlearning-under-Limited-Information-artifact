import os

import torch
import numpy as np
import copy
from sklearn.metrics import classification_report, f1_score
from torch.utils.data import TensorDataset, DataLoader

from approx_retraining import approx_retraining



def evaluate_unlearning(model, remove_x, remove_y,random_y ,x_train, y_train,test_x, test_y,lab):
    device = remove_x.device  # å‡è®¾ remove_x æ˜¯ cuda çš„
    model = model.to(device)  # æŠŠæ¨¡å‹ä¹Ÿæ¬åˆ° cuda ä¸Š
    new_model = copy.deepcopy(model)

    unlearned_model = update_unlearn(new_model, remove_x, remove_y,random_y,x_train, y_train)
    if test_x is not None and test_y is not None:
        from torch.utils.data import DataLoader, TensorDataset
        unlearned_model.eval()
        test_dataset = TensorDataset(test_x, test_y)
        test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)

        correct = 0
        total = 0
        with torch.no_grad():
            for x_batch, y_batch in test_loader:
                x_batch = x_batch.to(device)
                y_batch = y_batch.to(device)
                outputs = unlearned_model(x_batch)
                _, preds = outputs.max(1)
                correct += (preds == y_batch).sum().item()
                total += y_batch.size(0)

        acc = correct / total
        print(f"âœ… Updated model accuracy on test set: {acc:.4f}")

    acc_before_fix, acc_after_fix, is_diverged = evaluate_model_diff(
        model, unlearned_model, test_x, test_y, 'D:/pythonp/Macbsntb/resnet-18',lab
    )
    return acc_before_fix, acc_after_fix, is_diverged



def evaluate_model_diff(model, unlearned_model, test_x, test_y, log_path,lab,batch_size=128, device='cuda'):
    test_dataset = TensorDataset(test_x, test_y)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    lab1=lab
    model.eval()
    unlearned_model.eval()

    # å‡†å¤‡ä¿å­˜æ‰€æœ‰é¢„æµ‹ç»“æœå’ŒçœŸå®æ ‡ç­¾
    all_labels = []
    all_preds_model = []
    all_preds_unlearned = []

    with torch.no_grad():
        for x_batch, y_batch in test_loader:
            x_batch = x_batch.to(device)
            y_batch = y_batch.to(device)

            outputs_model = model(x_batch)
            outputs_unlearned = unlearned_model(x_batch)

            _, pred_model = outputs_model.max(1)
            _, pred_unlearned = outputs_unlearned.max(1)

            # ç´¯åŠ æ‰€æœ‰é¢„æµ‹å’Œæ ‡ç­¾ï¼ˆè½¬ä¸ºcpuï¼Œä¾¿äºåç»­ä½¿ç”¨ sklearnï¼‰
            all_labels.extend(y_batch.cpu().tolist())
            all_preds_model.extend(pred_model.cpu().tolist())
            all_preds_unlearned.extend(pred_unlearned.cpu().tolist())

    # æ•´ä½“å‡†ç¡®ç‡
    acc_model = sum([p == t for p, t in zip(all_preds_model, all_labels)]) / len(all_labels)
    acc_unlearned = sum([p == t for p, t in zip(all_preds_unlearned, all_labels)]) / len(all_labels)
    acc_diff = acc_model - acc_unlearned

    print(f"âœ… åŸæ¨¡å‹å‡†ç¡®ç‡: {acc_model:.4f}")
    print(f"âœ… é—å¿˜åæ¨¡å‹å‡†ç¡®ç‡: {acc_unlearned:.4f}")
    print(f"âš–ï¸ ä¸¤ä¸ªæ¨¡å‹æ•´ä½“ç²¾åº¦ä¹‹å·®: {acc_diff:.4f}")

    # ğŸ¯ ç›®æ ‡ç±» lab ç²¾åº¦å¯¹æ¯”
    lab_correct_model = sum([p == t for p, t in zip(all_preds_model, all_labels) if t ==  lab1])
    lab_correct_unlearned = sum([p == t for p, t in zip(all_preds_unlearned, all_labels) if t ==  lab1])
    lab_total = all_labels.count(lab1)

    lab_acc_model = lab_correct_model / lab_total if lab_total > 0 else 0.0
    lab_acc_unlearned = lab_correct_unlearned / lab_total if lab_total > 0 else 0.0
    lab_acc_diff = lab_acc_model - lab_acc_unlearned
    lab_f1_model = f1_score(all_labels, all_preds_model, labels=[lab1], average='macro')
    lab_f1_unlearned = f1_score(all_labels, all_preds_unlearned, labels=[lab1], average='macro')
    lab_f1_diff = lab_f1_model - lab_f1_unlearned

    print(f"\nğŸ¯ ç›®æ ‡ç±» {lab1} åœ¨åŸæ¨¡å‹ä¸Šçš„ç²¾åº¦: {lab_acc_model:.4f}ï¼ŒF1: {lab_f1_model:.4f}")
    print(f"ğŸ¯ ç›®æ ‡ç±» { lab1} åœ¨é—å¿˜æ¨¡å‹ä¸Šçš„ç²¾åº¦: {lab_acc_unlearned:.4f}ï¼ŒF1: {lab_f1_unlearned:.4f}")
    print(f"ğŸ¯ ç²¾åº¦å·®: {lab_acc_diff:.4f}ï¼ŒF1å·®: {lab_f1_diff:.4f}")

    # ä½¿ç”¨ sklearn è·å– per-class ç²¾åº¦ã€å¬å›ã€F1
    print("\nğŸ“Š åŸæ¨¡å‹åˆ†ç±»æŠ¥å‘Šï¼š")
   # print(classification_report(all_labels, all_preds_model, digits=4))

    print("\nğŸ“Š é—å¿˜åæ¨¡å‹åˆ†ç±»æŠ¥å‘Šï¼š")
    #print(classification_report(all_labels, all_preds_unlearned, digits=4))

    # ä¿å­˜æ—¥å¿—
    log_path = os.path.join(log_path, "log.txt")
    with open(log_path, 'a') as f:
        f.write(
            f'Original Model Acc: {acc_model:.4f}, Unlearned Model Acc: {acc_unlearned:.4f}, Acc Diff: {acc_diff:.4f}\n')
        f.write(
            f'Target Class {lab} Acc (Original): {lab_acc_model:.4f}, (Unlearned): {lab_acc_unlearned:.4f}, Diff: {lab_acc_diff:.4f}\n')
    return 1

def update_unlearn(model, remove_x, remove_y,random_y,
                   hvp_x, hvp_y, order=2, lr=0.002, device='cuda'):

    model.zero_grad()

    updated_model = approx_retraining(
        model=model,
        remove_x=remove_x,
        remove_y=remove_y,
        random_y=random_y,
        order=order,
        hvp_x=hvp_x,
        hvp_y=hvp_y,
        verbose=True,
        unlearn_type=0
    )

    return updated_model

